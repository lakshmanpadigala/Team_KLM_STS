{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers\n!pip install sentencepiece\n!pip install torchmetrics\n!pip install pytorch_lightning\n!pip install wandb\n!wandb.login()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-15T10:26:08.875904Z","iopub.execute_input":"2023-03-15T10:26:08.876300Z","iopub.status.idle":"2023-03-15T10:26:58.137956Z","shell.execute_reply.started":"2023-03-15T10:26:08.876260Z","shell.execute_reply":"2023-03-15T10:26:58.136733Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.26.1)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.13.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (23.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.9.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.4)\nRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.11.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.14)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.12.7)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (0.1.97)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: torchmetrics in /opt/conda/lib/python3.7/site-packages (0.11.1)\nRequirement already satisfied: torch>=1.8.1 in /opt/conda/lib/python3.7/site-packages (from torchmetrics) (1.13.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torchmetrics) (4.4.0)\nRequirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.7/site-packages (from torchmetrics) (1.21.6)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from torchmetrics) (23.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: pytorch_lightning in /opt/conda/lib/python3.7/site-packages (1.9.3)\nRequirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.7/site-packages (from pytorch_lightning) (6.0)\nRequirement already satisfied: lightning-utilities>=0.6.0.post0 in /opt/conda/lib/python3.7/site-packages (from pytorch_lightning) (0.7.1)\nRequirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from pytorch_lightning) (4.4.0)\nRequirement already satisfied: fsspec[http]>2021.06.0 in /opt/conda/lib/python3.7/site-packages (from pytorch_lightning) (2023.1.0)\nRequirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.7/site-packages (from pytorch_lightning) (4.64.1)\nRequirement already satisfied: packaging>=17.1 in /opt/conda/lib/python3.7/site-packages (from pytorch_lightning) (23.0)\nRequirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.7/site-packages (from pytorch_lightning) (1.21.6)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from pytorch_lightning) (1.13.0)\nRequirement already satisfied: torchmetrics>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from pytorch_lightning) (0.11.1)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.7/site-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (3.8.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (2.28.2)\nRequirement already satisfied: importlib-metadata>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from lightning-utilities>=0.6.0.post0->pytorch_lightning) (4.11.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (6.0.4)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.3)\nRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (2.1.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (22.2.0)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (0.13.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.8.2)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (4.0.2)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.0.0->lightning-utilities>=0.6.0.post0->pytorch_lightning) (3.11.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (1.26.14)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2022.12.7)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (3.4)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: wandb in /opt/conda/lib/python3.7/site-packages (0.13.10)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.7/site-packages (from wandb) (1.3.2)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from wandb) (4.4.0)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.28.2)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from wandb) (6.0)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.7/site-packages (from wandb) (0.1.2)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (8.1.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from wandb) (59.8.0)\nRequirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.1.30)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.15.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from Click!=8.0.0,>=7.0->wandb) (4.11.4)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.0.10)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (1.26.14)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->Click!=8.0.0,>=7.0->wandb) (3.11.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m/bin/bash: -c: line 1: syntax error: unexpected end of file\n","output_type":"stream"}]},{"cell_type":"code","source":"import json\nfrom functools import partial\nimport re\n#from transformers import XLMRobertaTokenizer, XLMRobertaModel\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nfrom torch import nn\nfrom torch.optim import AdamW\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport torchmetrics\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nimport pytorch_lightning as pl\nfrom transformers import BertTokenizer, BertModel\nfrom pytorch_lightning.loggers import WandbLogger\n\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2023-03-15T10:43:19.045811Z","iopub.execute_input":"2023-03-15T10:43:19.046218Z","iopub.status.idle":"2023-03-15T10:43:19.055181Z","shell.execute_reply.started":"2023-03-15T10:43:19.046182Z","shell.execute_reply":"2023-03-15T10:43:19.054015Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nencoder = BertModel.from_pretrained('bert-base-uncased')\n# tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n# encoder = AutoModel.from_pretrained('bert-base-cased')\n#tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n#encoder = XLMRobertaModel.from_pretrained('xlm-roberta-base')\n# encoder","metadata":{"execution":{"iopub.status.busy":"2023-03-15T10:54:54.931763Z","iopub.execute_input":"2023-03-15T10:54:54.932594Z","iopub.status.idle":"2023-03-15T10:54:56.724444Z","shell.execute_reply.started":"2023-03-15T10:54:54.932548Z","shell.execute_reply":"2023-03-15T10:54:56.719305Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"EXP_NAME = 'expi_1'\nTRAIN_BATCH_SIZE = 2 \nDEV_BATCH_SIZE = 64\nACCUMULATE_GRAD = 8","metadata":{"execution":{"iopub.status.busy":"2023-03-15T10:43:23.226508Z","iopub.execute_input":"2023-03-15T10:43:23.227335Z","iopub.status.idle":"2023-03-15T10:43:23.235738Z","shell.execute_reply.started":"2023-03-15T10:43:23.227295Z","shell.execute_reply":"2023-03-15T10:43:23.232563Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"class SimilarityRegressor(nn.Module):\n    def __init__(self, encoder, embed_size=768, hidden_size=256):\n        super(SimilarityRegressor, self).__init__()\n\n        self.encoder = encoder\n        self.embed_size = embed_size\n        self.hidden_size = hidden_size\n        \n        self.linear1 = nn.Linear(self.embed_size, self.hidden_size)\n        self.activation1 = nn.LeakyReLU(negative_slope=0.1)\n        self.dropout1 = nn.Dropout(p=0.2)\n        self.linear2 = nn.Linear(2*self.hidden_size, self.hidden_size//2)\n        self.dropout2 = nn.Dropout(p=0.2)\n        self.activation2 = nn.LeakyReLU(negative_slope=0.1)\n        self.linear3 = nn.Linear(self.hidden_size//2, 1)\n        #self.activation3 = nn.Sigmoid()\n\n    def common_compute(self, x):\n#         x = self.encoder(**x)[0][:, 0]\n        x = self.encoder(x).last_hidden_state[:, 0, :]\n        x = self.linear1(x)\n        x = self.activation1(x)\n        x = self.dropout1(x)\n\n        return x\n    \n    def forward(self, x1, x2):\n        x1 = self.common_compute(x1)\n        x2 = self.common_compute(x2)\n        x = torch.cat([torch.abs(x1 - x2), (x1 + x2)], dim=-1)\n        x = self.linear2(x)\n        x = self.activation2(x)\n        x = self.dropout2(x)\n        x = self.linear3(x)\n        #x = 3*self.activation3(x) + 1\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:00:08.100844Z","iopub.execute_input":"2023-03-15T11:00:08.101548Z","iopub.status.idle":"2023-03-15T11:00:08.114911Z","shell.execute_reply.started":"2023-03-15T11:00:08.101508Z","shell.execute_reply":"2023-03-15T11:00:08.113910Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"class LitSimilarityRegressor(pl.LightningModule):\n    def __init__(self, encoder, embed_size=768, hidden_size=256):\n        super(LitSimilarityRegressor, self).__init__()\n        self.model = SimilarityRegressor(encoder, embed_size=embed_size, hidden_size=hidden_size)\n\n        self.train_loss = torchmetrics.MeanMetric(compute_on_step=True)\n        self.dev_loss = torchmetrics.MeanMetric(compute_on_step=False)\n        self.test_loss = torchmetrics.MeanMetric(compute_on_step=False)\n\n        self.train_mape = torchmetrics.MeanAbsolutePercentageError(compute_on_step=True)\n        self.dev_mape = torchmetrics.MeanAbsolutePercentageError(compute_on_step=False)\n        self.test_mape = torchmetrics.MeanAbsolutePercentageError(compute_on_step=False)\n\n        self.train_pcc = torchmetrics.PearsonCorrCoef(compute_on_step=True)\n        self.dev_pcc = torchmetrics.PearsonCorrCoef(compute_on_step=False)\n        self.test_pcc = torchmetrics.PearsonCorrCoef(compute_on_step=False)\n\n    def forward(self, x1, x2):\n        return self.model(x1, x2)\n\n    def configure_optimizers(self):\n        return AdamW(self.parameters(), lr=1e-5, betas=(0.9, 0.99), eps=1e-8, weight_decay=0.01)\n\n    def training_step(self, batch, batch_idx):\n        x1, x2, scores = batch\n        output = self(x1, x2)\n        loss = F.mse_loss(input=output, target=scores)\n\n        return {'loss': loss, 'preds': output, 'target': scores}\n\n    def validation_step(self, batch, batch_idx):\n        x1, x2, scores = batch\n        output = self(x1, x2)\n        loss = F.mse_loss(input=output, target=scores)\n\n        return {'loss': loss, 'preds': output, 'target': scores}\n    \n    def test_step(self, batch, batch_idx):\n        x1, x2, scores = batch\n        output = self(x1, x2)\n        loss = F.mse_loss(input=output, target=scores)\n\n        return {'loss': loss, 'preds': output, 'target': scores}\n\n    def predict_step(self, batch, batch_idx):\n        x1, x2, _ = batch\n        output = self(x1, x2)\n\n        return {'preds': output}\n\n    def training_step_end(self, outs):\n        loss = outs['loss']\n        preds = outs['preds']\n        target = outs['target']\n\n        self.log('train/step/loss', self.train_loss(loss))\n        self.log('train/step/mape', self.train_mape(preds, target))\n        self.log('train/step/pcc', self.train_pcc(torch.reshape(preds, (-1,)), torch.reshape(target, (-1,))))\n\n    def validation_step_end(self, outs):\n        loss = outs['loss']\n        preds = outs['preds']\n        target = outs['target']\n\n        self.dev_loss(loss)\n        self.dev_mape(preds, target)\n        self.dev_pcc(torch.reshape(preds, (-1,)), torch.reshape(target, (-1,)))\n\n    def test_step_end(self, outs):\n        loss = outs['loss']\n        preds = outs['preds']\n        target = outs['target']\n\n        self.test_loss(loss)\n        self.test_mape(preds, target)\n        self.test_pcc(torch.reshape(preds, (-1,)), torch.reshape(target, (-1,)))\n\n    def training_epoch_end(self, outs):\n        self.log('train/epoch/loss', self.train_loss)\n        self.log('train/epoch/mape', self.train_mape)\n        self.log('train/epoch/pcc', self.train_pcc)\n\n    def validation_epoch_end(self, outs):\n        self.log('dev/loss', self.dev_loss)\n        self.log('dev/mape', self.dev_mape)\n        self.log('dev/pcc', self.dev_pcc)\n    \n    def test_epoch_end(self, outs):\n        self.log('dev/loss', self.test_loss)\n        self.log('dev/mape', self.test_mape)\n        self.log('dev/pcc', self.test_pcc)","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:00:08.628509Z","iopub.execute_input":"2023-03-15T11:00:08.628873Z","iopub.status.idle":"2023-03-15T11:00:08.661825Z","shell.execute_reply.started":"2023-03-15T11:00:08.628839Z","shell.execute_reply":"2023-03-15T11:00:08.660039Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"model = LitSimilarityRegressor(encoder, embed_size=768, hidden_size=256)\n# model","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:00:09.073248Z","iopub.execute_input":"2023-03-15T11:00:09.074276Z","iopub.status.idle":"2023-03-15T11:00:09.097433Z","shell.execute_reply.started":"2023-03-15T11:00:09.074218Z","shell.execute_reply":"2023-03-15T11:00:09.096340Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"train_text = list()\ntrain_file = open('/kaggle/input/enginh-sts/en-train.txt','r')\ntrain_text_lines = train_file.readlines()\nfor line in train_text_lines:\n    s1,s2,score = line.split('\\t')\n    s1 = re.sub(r'[^a-zA-Z0-9\\s]', '', s1).lower()\n    s2 = re.sub(r'[^a-zA-Z0-9\\s]', '', s2).lower()\n    score = float(score.strip())\n    train_text.append([s1,s2,score])\nval_text = list()\nval_file = open('/kaggle/input/enginh-sts/en-val.txt','r')\nval_text_lines = val_file.readlines()\nfor line in val_text_lines:\n    s1,s2,score = line.split('\\t')\n    s1 = re.sub(r'[^a-zA-Z0-9\\s]', '', s1).lower()\n    s2 = re.sub(r'[^a-zA-Z0-9\\s]', '', s2).lower()\n    score = float(score.strip())\n    val_text.append([s1,s2,score])\ntest_text = list()\ntest_file = open('/kaggle/input/englist-sts-test/test-sentences-2017.txt','r')\ntest_text_lines = test_file.readlines()\nfor line in test_text_lines:\n    s1,s2,score = line.split('\\t')\n    s1 = re.sub(r'[^a-zA-Z0-9\\s]', '', s1).lower()\n    s2 = re.sub(r'[^a-zA-Z0-9\\s]', '', s2).lower()\n    score = float(score.strip())\n    test_text.append([s1,s2,score])","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:00:09.511465Z","iopub.execute_input":"2023-03-15T11:00:09.511810Z","iopub.status.idle":"2023-03-15T11:00:09.626561Z","shell.execute_reply.started":"2023-03-15T11:00:09.511779Z","shell.execute_reply":"2023-03-15T11:00:09.625463Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"val_text[0]","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:00:12.516809Z","iopub.execute_input":"2023-03-15T11:00:12.517831Z","iopub.status.idle":"2023-03-15T11:00:12.530614Z","shell.execute_reply.started":"2023-03-15T11:00:12.517777Z","shell.execute_reply":"2023-03-15T11:00:12.529278Z"},"trusted":true},"execution_count":57,"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"['an alarm in the offing on climatechange      extremeweather globalwarming eco',\n 'an alarm in the offing on climate change',\n 4.6]"},"metadata":{}}]},{"cell_type":"code","source":"def collate_fn(batch, tokenizer):\n    texts_1, texts_2, scores = list(), list(), list()\n    for sample in batch:\n\n        text1 = str(sample[0]).lower().strip()\n        text2 = str(sample[1]).lower().strip()\n        \n        score = torch.tensor([sample[2]])\n        texts_1.append(text1)\n        texts_2.append(text2)\n        scores.append(score)\n\n    texts_1 = tokenizer(texts_1, truncation=True, padding=True, return_tensors='pt')\n    texts_2 = tokenizer(texts_2, truncation=True, padding=True, return_tensors='pt')\n#     tokenized_1 = [tokenizer.encode(sent,add_special_tokens = True) for sent in texts_1]\n#     tokenized_2 = [tokenizer.encode(sent,add_special_tokens = True) for sent in texts_2]\n#     # Pad and truncate the tokenized sequences to a fixed length\n#     max_len = 64\n#     padded_1 = torch.nn.utils.rnn.pad_sequence([torch.tensor(seq[:max_len]) for seq in tokenized_1], batch_first=True)\n#     padded_2 = torch.nn.utils.rnn.pad_sequence([torch.tensor(seq[:max_len]) for seq in tokenized_2], batch_first=True)\n\n#     attention_mask_1 = (padded_1!=0).float()\n#     attention_mask_2 = (padded_2!=0).float()\n    \n    \n    #     print(\"From tokerizer:\",texts_1)\n    scores = torch.cat(scores, dim=0).unsqueeze(1)\n\n    return texts_1, texts_2, scores\n#     return padded_1,padded_2,scores","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:00:12.959913Z","iopub.execute_input":"2023-03-15T11:00:12.960633Z","iopub.status.idle":"2023-03-15T11:00:12.972016Z","shell.execute_reply.started":"2023-03-15T11:00:12.960595Z","shell.execute_reply":"2023-03-15T11:00:12.970497Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"collate_partial = partial(collate_fn, tokenizer=tokenizer)\ndataloader = DataLoader(train_text, batch_size=TRAIN_BATCH_SIZE, shuffle=True, collate_fn=collate_partial)\nnext(iter(dataloader))","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:00:18.338791Z","iopub.execute_input":"2023-03-15T11:00:18.339164Z","iopub.status.idle":"2023-03-15T11:00:18.363873Z","shell.execute_reply.started":"2023-03-15T11:00:18.339131Z","shell.execute_reply":"2023-03-15T11:00:18.362190Z"},"trusted":true},"execution_count":59,"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"({'input_ids': tensor([[  101,  1038,  1998,  1039,  2024,  2119,  3033,  1997,  3143, 25022,\n          11890, 16446,  2000,  1996,  6046,   102],\n         [  101,  2022,  2284,  2012,  2030,  2006,  2327,  1997,  2242,   102,\n              0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])},\n {'input_ids': tensor([[  101, 25548,  1038,  1998,  1039,  2024,  2145,  1999,  2701, 10425,\n           2007,  1996,  6046,   102],\n         [  101,  2191,  1037,  2143,  2030,  9982,  1997,  2242,   102,     0,\n              0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n         [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]])},\n tensor([[3.8000],\n         [0.0000]]))"},"metadata":{}}]},{"cell_type":"code","source":"class MLNSDataModule(pl.LightningDataModule):\n    def __init__(self, train_dataset, dev_dataset, test_dataset, train_batch_size, dev_batch_size, collate_fn, tokenizer):\n        super(MLNSDataModule, self).__init__()\n        self.train_dataset = train_dataset\n        self.dev_dataset = dev_dataset\n        self.test_dataset = test_dataset\n        self.train_batch_size = train_batch_size\n        self.dev_batch_size = dev_batch_size\n        self.collate_fn = collate_fn\n        self.tokenizer = tokenizer\n\n    def train_dataloader(self):\n        collate_partial = partial(self.collate_fn, tokenizer=self.tokenizer)\n        return DataLoader(self.train_dataset, shuffle=True, batch_size=self.train_batch_size, collate_fn=collate_partial)\n\n    def val_dataloader(self):\n        collate_partial = partial(self.collate_fn, tokenizer=self.tokenizer)\n        return DataLoader(self.dev_dataset, shuffle=False, batch_size=self.dev_batch_size, collate_fn=collate_partial)\n\n    def test_dataloader(self):\n        collate_partial = partial(self.collate_fn, tokenizer=self.tokenizer)\n        return DataLoader(self.test_dataset, shuffle=False, batch_size=self.dev_batch_size, collate_fn=collate_partial)\n\n    def predict_dataloader(self):\n        #return DataLoader(self.test_dataset, shuffle=False, batch_size=self.dev_batch_size, collate_fn=collate_partial)\n        return self.test_dataloader()","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:00:19.019304Z","iopub.execute_input":"2023-03-15T11:00:19.019676Z","iopub.status.idle":"2023-03-15T11:00:19.030355Z","shell.execute_reply.started":"2023-03-15T11:00:19.019643Z","shell.execute_reply":"2023-03-15T11:00:19.029210Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"data_module = MLNSDataModule(train_text, val_text, test_text, TRAIN_BATCH_SIZE, DEV_BATCH_SIZE, collate_fn=collate_fn, tokenizer=tokenizer)\ndata_module","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:00:19.949189Z","iopub.execute_input":"2023-03-15T11:00:19.950329Z","iopub.status.idle":"2023-03-15T11:00:19.965661Z","shell.execute_reply.started":"2023-03-15T11:00:19.950279Z","shell.execute_reply":"2023-03-15T11:00:19.964015Z"},"trusted":true},"execution_count":61,"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"<__main__.MLNSDataModule at 0x7fdd796e7f90>"},"metadata":{}}]},{"cell_type":"markdown","source":"**Training**","metadata":{}},{"cell_type":"code","source":"logger = WandbLogger(save_dir=EXP_NAME, project=EXP_NAME, log_model=False)\nlogger","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:00:22.079617Z","iopub.execute_input":"2023-03-15T11:00:22.080583Z","iopub.status.idle":"2023-03-15T11:00:22.095032Z","shell.execute_reply.started":"2023-03-15T11:00:22.080541Z","shell.execute_reply":"2023-03-15T11:00:22.093722Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/loggers/wandb.py:396: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n  \"There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse\"\n","output_type":"stream"},{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"<pytorch_lightning.loggers.wandb.WandbLogger at 0x7fde6b084b10>"},"metadata":{}}]},{"cell_type":"code","source":"checkpoint_callback = ModelCheckpoint(\n    dirpath='/kaggle/working/',\n    filename='{epoch}-{step}',\n    monitor='dev/pcc',\n    mode='max',\n    save_top_k=1,\n    verbose=True,\n    save_last=False,\n    save_weights_only=False,\n    every_n_epochs=1\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:00:22.993408Z","iopub.execute_input":"2023-03-15T11:00:22.993773Z","iopub.status.idle":"2023-03-15T11:00:23.009358Z","shell.execute_reply.started":"2023-03-15T11:00:22.993742Z","shell.execute_reply":"2023-03-15T11:00:23.005562Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"trainer = pl.Trainer(max_epochs=10,\n                     accumulate_grad_batches=ACCUMULATE_GRAD,\n                     accelerator='gpu',\n                     check_val_every_n_epoch=1, val_check_interval=0.25,\n                     enable_progress_bar=True,\n                     gradient_clip_val=0.25, track_grad_norm=2,\n                     enable_checkpointing=True,\n                     callbacks=[checkpoint_callback],\n                     logger=logger,\n                     enable_model_summary=True)\n\ntrainer","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:00:23.534431Z","iopub.execute_input":"2023-03-15T11:00:23.534772Z","iopub.status.idle":"2023-03-15T11:00:23.557744Z","shell.execute_reply.started":"2023-03-15T11:00:23.534741Z","shell.execute_reply":"2023-03-15T11:00:23.556524Z"},"trusted":true},"execution_count":64,"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"<pytorch_lightning.trainer.trainer.Trainer at 0x7fde6af1b9d0>"},"metadata":{}}]},{"cell_type":"code","source":"trainer.fit(model, datamodule=data_module)\n#trainer.fit(model, datamodule=data_module, ckpt_path=root_path+'model/epoch=3-step=571.ckpt')","metadata":{"execution":{"iopub.status.busy":"2023-03-15T11:00:25.188622Z","iopub.execute_input":"2023-03-15T11:00:25.189512Z","iopub.status.idle":"2023-03-15T11:00:30.407701Z","shell.execute_reply.started":"2023-03-15T11:00:25.189473Z","shell.execute_reply":"2023-03-15T11:00:30.404020Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:613: UserWarning: Checkpoint directory /kaggle/working exists and is not empty.\n  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d07d0d3cb9d14728adc539d6f04b0dd2"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:218: UserWarning: strategy=ddp_spawn and num_workers=0 may result in data loading bottlenecks. Consider setting num_workers>0 and persistent_workers=True\n  \"strategy=ddp_spawn and num_workers=0 may result in data loading bottlenecks.\"\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mProcessRaisedException\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_22/3762705327.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#trainer.fit(model, datamodule=data_module, ckpt_path=root_path+'model/epoch=3-step=571.ckpt')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lightning_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         call._call_and_handle_interrupt(\n\u001b[0;32m--> 609\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m         )\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\u001b[0m in \u001b[0;36mlaunch\u001b[0;34m(self, function, trainer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mnprocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_processes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0mstart_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         )\n\u001b[1;32m    119\u001b[0m         \u001b[0mworker_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;31m# Loop on join until it returns True or raises an exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n\\n-- Process %d terminated with the following error:\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0merror_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0moriginal_trace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mProcessRaisedException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfailed_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mProcessRaisedException\u001b[0m: \n\n-- Process 1 terminated with the following error:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\", line 248, in __getattr__\n    return self.data[item]\nKeyError: 'size'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 69, in _wrap\n    fn(i, *args)\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\", line 139, in _wrapping_function\n    results = function(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 650, in _fit_impl\n    self._run(model, ckpt_path=self.ckpt_path)\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1112, in _run\n    results = self._run_stage()\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1191, in _run_stage\n    self._run_train()\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1204, in _run_train\n    self._run_sanity_check()\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1276, in _run_sanity_check\n    val_loop.run()\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py\", line 199, in run\n    self.advance(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\", line 152, in advance\n    dl_outputs = self.epoch_loop.run(self._data_fetcher, dl_max_batches, kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py\", line 199, in run\n    self.advance(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 137, in advance\n    output = self._evaluation_step(**kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 234, in _evaluation_step\n    output = self.trainer._call_strategy_hook(hook_name, *kwargs.values())\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1494, in _call_strategy_hook\n    output = fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/strategies/ddp_spawn.py\", line 288, in validation_step\n    return self.model(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/distributed.py\", line 1040, in forward\n    output = self._run_ddp_forward(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/distributed.py\", line 1000, in _run_ddp_forward\n    return module_to_run(*inputs[0], **kwargs[0])\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/overrides/base.py\", line 110, in forward\n    return self._forward_module.validation_step(*inputs, **kwargs)\n  File \"/tmp/ipykernel_22/250285318.py\", line 33, in validation_step\n    output = self(x1, x2)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/tmp/ipykernel_22/250285318.py\", line 19, in forward\n    return self.model(x1, x2)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/tmp/ipykernel_22/1013392838.py\", line 28, in forward\n    x1 = self.common_compute(x1)\n  File \"/tmp/ipykernel_22/1013392838.py\", line 20, in common_compute\n    x = self.encoder(x).last_hidden_state[:, 0, :]\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\", line 967, in forward\n    input_shape = input_ids.size()\n  File \"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\", line 250, in __getattr__\n    raise AttributeError\nAttributeError\n"],"ename":"ProcessRaisedException","evalue":"\n\n-- Process 1 terminated with the following error:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\", line 248, in __getattr__\n    return self.data[item]\nKeyError: 'size'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 69, in _wrap\n    fn(i, *args)\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/strategies/launchers/multiprocessing.py\", line 139, in _wrapping_function\n    results = function(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 650, in _fit_impl\n    self._run(model, ckpt_path=self.ckpt_path)\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1112, in _run\n    results = self._run_stage()\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1191, in _run_stage\n    self._run_train()\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1204, in _run_train\n    self._run_sanity_check()\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1276, in _run_sanity_check\n    val_loop.run()\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py\", line 199, in run\n    self.advance(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\", line 152, in advance\n    dl_outputs = self.epoch_loop.run(self._data_fetcher, dl_max_batches, kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/loops/loop.py\", line 199, in run\n    self.advance(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 137, in advance\n    output = self._evaluation_step(**kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 234, in _evaluation_step\n    output = self.trainer._call_strategy_hook(hook_name, *kwargs.values())\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\", line 1494, in _call_strategy_hook\n    output = fn(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/strategies/ddp_spawn.py\", line 288, in validation_step\n    return self.model(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/distributed.py\", line 1040, in forward\n    output = self._run_ddp_forward(*inputs, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/distributed.py\", line 1000, in _run_ddp_forward\n    return module_to_run(*inputs[0], **kwargs[0])\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/overrides/base.py\", line 110, in forward\n    return self._forward_module.validation_step(*inputs, **kwargs)\n  File \"/tmp/ipykernel_22/250285318.py\", line 33, in validation_step\n    output = self(x1, x2)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/tmp/ipykernel_22/250285318.py\", line 19, in forward\n    return self.model(x1, x2)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/tmp/ipykernel_22/1013392838.py\", line 28, in forward\n    x1 = self.common_compute(x1)\n  File \"/tmp/ipykernel_22/1013392838.py\", line 20, in common_compute\n    x = self.encoder(x).last_hidden_state[:, 0, :]\n  File \"/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/transformers/models/bert/modeling_bert.py\", line 967, in forward\n    input_shape = input_ids.size()\n  File \"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\", line 250, in __getattr__\n    raise AttributeError\nAttributeError\n","output_type":"error"}]},{"cell_type":"code","source":"test_pred = trainer.predict(datamodule=data_module)\n\nall_outputs = list()\nfor batch_outputs in test_pred:\n    all_outputs.append(batch_outputs['preds'])\nall_outputs = torch.cat(all_outputs, dim=0)\n\nall_outputs.shape\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_result = trainer.test(datamodule=data_module)","metadata":{},"execution_count":null,"outputs":[]}]}