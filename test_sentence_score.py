# -*- coding: utf-8 -*-
"""test_sentence_score.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rgYGE8sdKzl8O4LQ_i2EzWpAkpy7Xfcv
"""

from nltk.tokenize import RegexpTokenizer
import numpy as np
import re
from keras.utils import pad_sequences
from keras.callbacks import EarlyStopping
import random,copy,string
import matplotlib.pyplot as plt
import nltk
import multiprocessing as mp
import random,copy,string
from scipy.stats import pearsonr
from tensorflow.python.keras import backend as K
from tensorflow.python.keras.models import Model
from tensorflow.python.keras.layers import Input, Conv1D,  MaxPool1D, Flatten
from tensorflow.python.keras.layers import Lambda, multiply, concatenate, Dense
from tensorflow.python.keras.callbacks import Callback
# from keras.optimizers import Adam
from keras.models import load_model
import tensorflow as tf
nltk.download('punkt')

# from google.colab import drive
# drive.mount('/content/drive')

word_to_index = dict()
index_to_word = dict()
index = 1
embed_dict = {}
embed_dict['oov'] = np.zeros(300)

best_model_path = '/content/drive/MyDrive/Colab Notebooks/Working/best_model_cnn.h5'
glove_filepath = '/content/drive/MyDrive/Colab Notebooks/Working/glove.6B.300d.txt'

with open(glove_filepath,'r') as f:  #/content/drive/MyDrive/NLP/glove.840B.300d.txt /content/drive/MyDrive/smai/glove/glove.6B.300d.txt
  for line in f:
    values = line.split(' ')
    word = values[0]
    vector = np.asarray(values[1:],'float32')
    embed_dict[word]=vector
    word_to_index[word] = index
    index_to_word[index] = word
    index+=1

word_to_index['oov'] = 0
index_to_word[0] = 'oov'

def generate_embedding_matrix(sentence):

    tokens_ = nltk.word_tokenize(sentence)
    tokens = []

    for i in tokens_:
      if i not in string.punctuation:
        tokens.append(i.lower())
    # sentence = sentence.lower()
    # sentence = re.sub('[^a-z0-9]',' ',sentence)
    # Tokenize the sentence and do the padding
    
    word_to_vec = []
    # Loop over the tokens and generate their embeddings
    for i, token in enumerate(tokens):
        if token in word_to_index:
            word_to_vec.append(word_to_index[token])
        else:
            word_to_vec.append(word_to_index['oov'])

    # print(word_to_vec)

    word_to_vec = np.array(word_to_vec)
    word_to_vec=word_to_vec.reshape(1,word_to_vec.shape[0])
    word_to_vec = pad_sequences(word_to_vec, maxlen=40, truncating='post', padding='post')

    # Initialize an empty matrix with shape (40, 300)
    embedding_matrix = np.zeros((40, 300))
    
    # Loop over the tokens and generate their embeddings
    for i, token in enumerate(word_to_vec[0]):
        embedding_matrix[i, :] = embed_dict[index_to_word[token]]
    return embedding_matrix

def _lossfunction(y_true,y_pred):
  ny_true = y_true[:,1] + 2*y_true[:,2] + 3*y_true[:,3] + 4*y_true[:,4] + 5*y_true[:,5]
  ny_pred = y_pred[:,1] + 2*y_pred[:,2] + 3*y_pred[:,3] + 4*y_pred[:,4] + 5*y_pred[:,5]
  my_true = K.mean(ny_true)
  my_pred = K.mean(ny_pred)
  var_true = (ny_true - my_true)**2
  var_pred = (ny_pred - my_pred)**2
  return -K.sum((ny_true-my_true)*(ny_pred-my_pred),axis=-1) / (K.sqrt(K.sum(var_true,axis=-1)*K.sum(var_pred,axis=-1)))

loaded_model = load_model(best_model_path,custom_objects={'_lossfunction': _lossfunction})

inp1 = input('Enter first sentence:')
inp2 = input('Enter second sentence:')
emd1,emd2 = list(),list()

emd1.append(generate_embedding_matrix(inp1))
emd2.append(generate_embedding_matrix(inp2))

emd1 = np.array(emd1)
emd2 = np.array(emd2)

predictionclasses = list(loaded_model.predict([emd1,emd2],verbose=0))
prediction = np.dot(np.array(predictionclasses),np.arange(6))

print("STS Score Predicted:",prediction[0])