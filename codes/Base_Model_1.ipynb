{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5Pq-L3u5PKY",
        "outputId": "669e53d5-1e62-41bf-b088-5dbe4d7db30f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.0-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.2-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.2 tokenizers-0.13.2 transformers-4.27.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "sktUU_hs87oV"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_text = list()\n",
        "train_scores = list()\n",
        "train_file = open('/content/en-train.txt','r')\n",
        "train_text_lines = train_file.readlines()\n",
        "for line in train_text_lines:\n",
        "    s1,s2,score = line.split('\\t')\n",
        "    s1 = re.sub(r'[^a-zA-Z0-9\\s]', '', s1).lower()\n",
        "    s2 = re.sub(r'[^a-zA-Z0-9\\s]', '', s2).lower()\n",
        "    train_scores.append(float(score.strip()))\n",
        "    train_text.append([s1,s2])\n",
        "val_text = list()\n",
        "val_scores = list()\n",
        "val_file = open('/content/en-val.txt','r')\n",
        "val_text_lines = val_file.readlines()\n",
        "for line in val_text_lines:\n",
        "    s1,s2,score = line.split('\\t')\n",
        "    s1 = re.sub(r'[^a-zA-Z0-9\\s]', '', s1).lower()\n",
        "    s2 = re.sub(r'[^a-zA-Z0-9\\s]', '', s2).lower()\n",
        "    val_scores.append(float(score.strip()))\n",
        "    val_text.append([s1,s2])\n",
        "test_text = list()\n",
        "test_scores = list()\n",
        "test_file = open('/content/test-sentences-2017.txt','r')\n",
        "test_text_lines = test_file.readlines()\n",
        "for line in test_text_lines:\n",
        "    s1,s2,score = line.split('\\t')\n",
        "    s1 = re.sub(r'[^a-zA-Z0-9\\s]', '', s1).lower()\n",
        "    s2 = re.sub(r'[^a-zA-Z0-9\\s]', '', s2).lower()\n",
        "    test_scores.append(float(score.strip()))\n",
        "    test_text.append([s1,s2])"
      ],
      "metadata": {
        "id": "FS2FoPhG8inW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_scores),len(test_scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2h9lOIf3XJj",
        "outputId": "dd986d49-b768-4ce5-e559-413d0998d691"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13365, 250)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_text[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gq7OBmrg52yk",
        "outputId": "c51dceac-fa7b-4dea-fff0-f6ea64acacf9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['umm the california was a cargo ship i dont care what the article says',\n",
              "  'the california was a cargo ship'],\n",
              " ['how can i prepare this old exterior wall for painting',\n",
              "  'how do i prepare this exterior concrete wall for paint'],\n",
              " ['the man is playing the guitar', 'a man is playing guitar'],\n",
              " ['it was a final test before delivering the missile to the armed forces',\n",
              "  'state radio said it was the last test before the missile was delivered to the armed forces'],\n",
              " ['this does not fully answer your question but it is certainly one consideration',\n",
              "  'i am a phd student in computational science which is an interdisciplinary major spanning mathematics computer science and engineerning'],\n",
              " ['a longedhaired cat with its eyes closed',\n",
              "  'a close up of a cat with its eyes closed'],\n",
              " ['the venture owns five oil refineries and more than 2100 filling stations in russia and ukraine',\n",
              "  'tnkbp has six oil producing units five refineries and 2100 filling stations'],\n",
              " ['we welcome the arrangements proposed in this respect by the preparatory committee and adopted by the general assembly regarding procedures for the registration of nongovernmental organisations for involvement in the preparatory process and the special session in september 2001',\n",
              "  'we are pleased with proposed arrangements in this respect by the committee preparatory and decided by the general assembly with regard to the mode of accreditation of the governmental organizations with the preparatory process and the extraordinary session of september 2001'],\n",
              " ['paul themba nyathi an mdc spokesman said the avenues clinic was being targeted because it was treating opposition supporters',\n",
              "  'mdc spokesperson paul themba nyathi said the avenues clinic was targeted because it was handling opposition supporters'],\n",
              " ['a young woman is playing the flute',\n",
              "  'a woman is washing off the top of a freezer']]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GF5_yUcqsjcK",
        "outputId": "841aa174-f04f-40f1-a36f-393d3cbd0a29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load the BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "encoder = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "pred_scores = list()\n",
        "for i in range(len(train_text)):\n",
        "  tokens_1 = tokenizer(train_text[i][0], padding=True, truncation=True, return_tensors='pt')\n",
        "  input_ids = tokens_1['input_ids']\n",
        "  attention_mask = tokens_1['attention_mask']\n",
        "\n",
        "  with torch.no_grad():\n",
        "    embeddings_1 = encoder(input_ids, attention_mask=attention_mask)\n",
        "    embeddings_1 = embeddings_1.last_hidden_state[:, 0, :]\n",
        "\n",
        "  tokens_2 = tokenizer(train_text[i][1], padding=True, truncation=True, return_tensors='pt')\n",
        "  input_ids = tokens_2['input_ids']\n",
        "  attention_mask = tokens_2['attention_mask']\n",
        "\n",
        "  with torch.no_grad():\n",
        "    embeddings_2 = encoder(input_ids, attention_mask=attention_mask)\n",
        "    embeddings_2 = embeddings_2.last_hidden_state[:, 0, :]\n",
        "  \n",
        "  # print(type(embeddings_1))\n",
        "  cos = nn.CosineSimilarity(dim=1)\n",
        "  pred_scores.append(cos(embeddings_1,embeddings_2)[0].item())\n",
        "  # cos_sim = f.cosine_similarity(embeddings_1.unsqueeze(0), embeddings_2.unsqueeze(0))\n",
        "\n",
        "  # # pred_scores.append(5*(cos_sim.item()))\n",
        "  # print(cos_sim.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import pearsonr\n",
        "pcc, p_value = pearsonr(train_scores,pred_scores )\n",
        "\n",
        "# Print the PCC\n",
        "print(\"PCC: {:.3f}\".format(pcc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3109a1lN5ylD",
        "outputId": "7069e04c-9883-459f-947b-08dede86831f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PCC: 0.343\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# # Load the BERT tokenizer and model\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# encoder = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# # Define the input texts\n",
        "# texts = [\"Hello, how are you doing today?\", \"What's the weather like outside?\", \"The quick brown fox jumps over the lazy dog.\"]\n",
        "\n",
        "# # Tokenize the texts and get the input IDs and attention masks\n",
        "# tokens = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
        "# input_ids = tokens['input_ids']\n",
        "# attention_mask = tokens['attention_mask']\n",
        "\n",
        "# # Generate the BERT embeddings\n",
        "# with torch.no_grad():\n",
        "#     embeddings = encoder(input_ids, attention_mask=attention_mask)\n",
        "#     embeddings = embeddings.last_hidden_state[:, 0, :]\n",
        "    \n",
        "# # Print the embeddings\n",
        "# print(embeddings)\n"
      ],
      "metadata": {
        "id": "L-1i0jPa8Tn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5bzW1uTz8_1S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}