{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import string\n",
        "import nltk\n",
        "import re\n",
        "import numpy as np\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "string.punctuation\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "porter_stemmer = PorterStemmer()\n",
        "wordnet_lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oCQNDhE6Spz",
        "outputId": "c99a6713-e8fa-4051-9ffb-398e53020ce6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def word_extraction(sentence):    \n",
        "   ignore = ['a', \"the\", \"is\"]    \n",
        "   words = re.sub(\"[^\\w]\", \" \",  sentence).split()    \n",
        "   cleaned_text = [w.lower() for w in words if w not in ignore]    \n",
        "   return cleaned_text"
      ],
      "metadata": {
        "id": "u-pA4qEFHYfU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(sentence):    \n",
        "   words = re.sub(\"[^\\w]\", \" \",  sentence).split()    \n",
        "   words = [w.lower() for w in words if w not in stopwords]  \n",
        "   tokens = [i for i in words if not i.isdigit()]\n",
        "   lemm_text_tokens = [wordnet_lemmatizer.lemmatize(word) for word in tokens]    \n",
        "   return lemm_text_tokens"
      ],
      "metadata": {
        "id": "9g21VEETEY3X"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenize('Hey! I am a boy. from room 3'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdEhULX7_eJ9",
        "outputId": "93e7451e-dcba-499c-c511-449da325c4df"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hey', 'i', 'boy', 'room']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oVJNjG54P6c",
        "outputId": "921bfe45-f1bc-45cf-8d99-d420eef6084f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['umm', 'california', 'cargo', 'ship', 'i', 'dont', 'care', 'article', 'say', 'the', 'california', 'cargo', 'ship', 'how', 'i', 'prepare', 'old', 'exterior', 'wall', 'painting', 'how', 'i', 'prepare', 'exterior', 'concrete', 'wall', 'paint', 'the', 'man', 'playing', 'guitar', 'a', 'man', 'playing', 'guitar', 'it', 'final', 'test', 'delivering', 'missile', 'armed', 'force', 'state', 'radio', 'said', 'last', 'test', 'missile', 'delivered', 'armed', 'force', 'this', 'fully', 'answer', 'question', 'certainly', 'one', 'consideration', 'i', 'phd', 'student', 'computational', 'science', 'interdisciplinary', 'major', 'spanning', 'mathematics', 'computer', 'science', 'engineerning', 'a', 'longed', 'haired', 'cat', 'eye', 'closed', 'a', 'close', 'cat', 'eye', 'closed', 'the', 'venture', 'owns', 'five', 'oil', 'refinery', 'filling', 'station', 'russia', 'ukraine', 'tnk', 'bp', 'six', 'oil', 'producing', 'unit', 'five', 'refinery', 'filling']\n"
          ]
        }
      ],
      "source": [
        "filename = '/content/en-train.txt'\n",
        "with open(filename,'r') as file_ptr:\n",
        "  data = file_ptr.read()\n",
        "  sentencelist=sent_tokenize(data)\n",
        "  tokens=[]\n",
        "  for string in sentencelist:\n",
        "    tokens.extend(tokenize(string))\n",
        "print(tokens[:100])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ncl3O1bGFFDQ",
        "outputId": "970bed77-ad2b-40b5-dd6f-51e975f5399c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "184304\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(set(tokens)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5uUJzn2FFbb",
        "outputId": "04cc76ad-cade-4e61-a9c2-67d8a04da230"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15607\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "vocab = list(set(tokens))\n",
        "vocab_index_dict=dict()\n",
        "word_count = len(vocab)\n",
        "for i,word in enumerate(vocab):                                 \n",
        "    vocab_index_dict[word] = i\n",
        "def generate_bow(sentence):        \n",
        "  # print(\"Word List for Document \\n{0} \\n\".format(vocab))       \n",
        "  bag_vector = np.zeros(word_count+1)\n",
        "  words = tokenize(sentence)        \n",
        "  for w in words:\n",
        "    idx = vocab_index_dict.get(w,word_count)           \n",
        "    bag_vector[idx]=1                           \n",
        "  print(\"{0}\\n{1}\\n\".format(sentence,np.array(bag_vector)))\n",
        "  return bag_vector"
      ],
      "metadata": {
        "id": "1-mPV1qnFIep"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_bow('Umm the California was a cargo ship I dont care what the article says.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsL7zWLwFmF-",
        "outputId": "0c1bcf26-f7bb-41e3-afe0-e671276d6616"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Umm the California was a cargo ship I dont care what the article says.\n",
            "[0. 0. 0. ... 0. 0. 0.]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., ..., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VUWPuMA7OzwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "def get_cosine_similarity(s1,s2):\n",
        "  a = generate_bow(s1)\n",
        "  b = generate_bow(s2)\n",
        "  cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
        "  # print(\"Cosine:\", cos_sim*5) \n",
        "  return cos_sim*5\n"
      ],
      "metadata": {
        "id": "jM-iHATeLkfU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(filename) as f:\n",
        "    lines = [line.rstrip('\\n') for line in f]\n",
        "s1_lines,s2_lines,actual_scores,predicted_scores = list(),list(),list(),list()\n",
        "for line in lines:\n",
        "  s1,s2,score = line.split('\\t')\n",
        "  s1_lines.append(s1)\n",
        "  s2_lines.append(s2)\n",
        "  actual_scores.append(eval(score))\n",
        "  y =  get_cosine_similarity(s1,s2)\n",
        "  predicted_scores.append(y)"
      ],
      "metadata": {
        "id": "kZBD1i4uJlCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import pearsonr\n",
        "actual_scores = [int(i) for i in actual_scores]\n",
        "pcc, p_value = pearsonr(actual_scores,predicted_scores)\n",
        "# Print the PCC\n",
        "print(\"PCC: {:.3f}\".format(pcc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMYGfAi2Q9Pr",
        "outputId": "22183f32-00ac-4ce3-9c82-03aacd8be8de"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PCC: 0.627\n"
          ]
        }
      ]
    }
  ]
}